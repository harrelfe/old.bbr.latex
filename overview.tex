\chapter{General Overview of Biostatistics} \katz{1-3}  \ems{2} \abd{1.1,p.23-4}\label{chap:overview}

\quoteit{There are no routine statistical questions, only questionable
  statistical routines.}{Sir David R.\ Cox}

\quoteit{It's much easier to get a \emph{result} than it is to get an
  \emph{answer}.}{Christie Aschwanden, \texttt{FireThirtyEight}}

\section{What is Biostatistics?}\label{sec:overview-biostat}
\bi
\item Statistics applied to biomedical problems
\item Decision making in the face of uncertainty or variability
\item Design and analysis of experiments; detective work in
  observational studies (in epidemiology, outcomes research, etc.)
\item Attempt to remove bias or find alternative explanations to those
  posited by researchers with vested interests
\item Experimental design, measurement, description, statistical graphics,
  data analysis, inference
\ei

To optimize its value, biostatistics needs to be fully integrated into
biomedical research and we must recognize that experimental design and
execution (e.g., randomization and masking) are all important.

\subsection{Fundamental Principles of Statistics}
\bi
\item Use methods grounded in theory or extensive simulation
\item Understand uncertainty
\item Design experiments to maximize information and understand
  sources of variability
\item Use all information in data during analysis
\item Use discovery and estimation procedures not likely to claim that
  noise is signal
\item Strive for optimal quantification of evidence about effects
\item Give decision makers the inputs (\emph{other} than the utility
  function\footnote{The utility function is also called the loss or
    cost function.  It specifies, for example, the damage done by
    making various decisions such as treating patients who don't have
    the disease or failing to treat those who do.  The optimum Bayes
    decision is the one that minimizes expected loss.  This decision
    conditions on full information and uses for example predicted risk
    rather than whether or not the predicted risk is high.}) that
  optimize decisions
\item Present information in ways that are intuitive, maximize
  information content, and are correctly perceived
\ei


\section{Types of Data Analysis and Inference}\label{sec:overview-datatypes}
 \bi
 \item Description: what happened to \emph{past} patients
 \item Inference from specific (a sample) to general (a population)
 \bi
  \item Hypothesis testing: test a hypothesis about population or
   long-run effects
  \item Estimation: approximate a population or long term average
   quantity
  \item Prediction: predict the responses of other patients \emph{like yours}
   based on analysis of patterns of responses in your patients
  \ei
\ei

\section{Types of Measurements by Their Role in the Study} \katz{3} \abd{1.3}\label{sec:overview-measurement-role}
\bi
 \item Response variable (clinical endpoint, final lab measurements,
   etc.)
 \item Independent variable (predictor or descriptor variable) ---
   something measured when a patient begins to be studied, before the
   response; often not controllable by 
   investigator, e.g. sex, weight, height, smoking history
 \item Adjustment variable (confounder) --- a variable not of major
   interest but one needing accounting for because it explains an
   apparent effect of a variable of major interest or because it
   describes heterogeneity in severity of risk factors across patients
 \item Experimental variable, e.g. the treatment or dose to which a
   patient is randomized; this is an independent variable under the
   control of the researcher
 \ei

\begin{table}[h!]
 \caption{Common alternatives for describing independent and response variables}
\begin{center}
\begin{tabular}{ll} \hline \hline
Response variable & Independent variable \\ \hline
Outcome variable & Exposure variable \\
Dependent variable & Predictor variable \\
$y$-variables &  $x$-variable \\
Case-control group &  Risk factor \\
 & Explanatory variable \\ \hline \hline
\end{tabular}
\end{center}
\end{table}

\subsection{Proper Response Variables}\label{overview-yproper}
It is too often the case that researchers concoct response variables $Y$
in such a way that makes the variables \emph{seem} to be easy to
interpret, but which contain several hidden problems:
\bi
\item $Y$ may be a categorization/dichotomization of an underlying
  continuous response variable.  The cutpoint used for the
  dichtomization is never consistent with data (see
  Figure~\ref{fig:info-thresholds}), 
  is arbitrary (P.~\pageref{pg:info-gia14opt}), and causes a huge loss of
  statistical information and power (P.~\pageref{pg:info-fed09con}).
\item $Y$ may be based on a change in a subject's condition whereas
  what is truly important is the subject's most recent condition
  (P.~\pageref{pg:change-anova}).
\item $Y$ may be based on change when the underlying variable is not
  monotonically related to the ultimate outcome, indicating that
  positive change is good for some subjects and bad for others
  (Fig.~\ref{fig:change-suppcr}).
\ei
A proper response variable that optimizes power is one that
\bi
\item Captures the underlying structure or process
\item Has low measurement error
\item Has the highest resolution available, e.g.
 \bi
 \item is continuous if the underlying measurement is continuous
 \item is ordinal with several categories if the underlying
   measurement is ordinal
 \item is binary only if the underlying process is truly
   all-or-nothing
 \ei
\item Has the same interpretation for every type of subject, and
  especially has a direction such that higher values are always good
  or always bad
\ei

\section{Types of Measurements According to Coding} \katz{3} \ems{2.2}\abd{1.3}\label{sec:overview-coding}
\bi
 \item Binary: yes/no, present/absent
 \item Categorical (nominal, polytomous, discrete): more than 2 values
   that are not necessarily in special order
 \item Ordinal: a categorical variable whose possible values are in a
   special order, e.g., by severity of symptom or disease; spacing
   between categories is not assumed to be useful
  \bi
  \item Ordinal variables that are not continuous often have heavy
    ties at one or more values requiring the use of statistical
    methods that allow for strange distributions and handle ties well
  \item Continuous are also ordinal but ordinal variables may or may
    not be continuous
  \ei
 \item Count: a discrete variable that (in theory) has no upper limit, e.g. the number of ER visits in a day, the number of traffic accidents in a month
 \item Continuous: a numeric variable having many possible values
   representing an underlying spectrum
 \item Continuous variables have the most statistical information
  (assuming the raw values are used in the data analysis) and
  are usually the easiest to standardize across hospitals
 \item Turning continuous variables
  into categories by using intervals of values is arbitrary and
  requires more patients to yield the same statistical information
  (precision or power)
 \item Errors are not reduced by categorization unless that's the only
   way to get a subject to answer the question (e.g.,
 income\footnote{But note how the Census Bureau tries to maximize the
 information collected.  They first ask for income in
 dollars.  Subjects refusing to answer are asked to choose from among
 10 or 20 categories.  Those not checking a category are asked to
 choose from fewer categories.})
\ei

\section{Choose $Y$ to Maximize Statistical Information, Power, and Interpretability}\label{sec:overview-ychoice}
The outcome (dependent) variable $Y$ should be a high-information \blog{ordinal-info}
measurement that is relevant to the subject at hand.  The information
provided by an analysis, and statistical power and precision, are
strongly influenced by characteristics of $Y$ in addition to the
effective sample size.
  \bi
  \item Noisy $Y \rightarrow$ variance $\uparrow$, effect of
    interest $\downarrow$
  \item Low information content/resolution also $\rightarrow$ power
    $\downarrow$ 
  \item Minimum information $Y$: binary outcome
  \item Maximum information $Y$: continuous response with almost no
    measurement error
    \bi
    \item Example: measure systolic blood pressure (SBP) well and
      average 5 readings 
    \ei
  \item Intermediate: ordinal $Y$ with a few well-populated levels
  \ei

\subsection{Information Content}\label{sec:overview-info-content}
  \bi
  \item Binary $Y$: 1 bit
    \bi
    \item all--or--nothing
    \item no gray zone, close calls
    \item often arbitrary
    \ei
  \item SBP: $\approx$ 5 bits
    \bi
    \item range 50-250mmHg (7 bits)
    \item accurate to nearest 4mmHg (2 bits)
    \ei
  \item Time to binary event: if proportion of subjects having event is
    small, is effectively a binary endpoint
    \bi
    \item becomes truly continuous and yields high power if proportion
      with events much greater than $\frac{1}{2}$, if time to event is
      clinically meaningful
    \item if there are multiple events, or you pool events of
      different severities, time to first event loses information
    \ei
  \ei
  
\subsection{Dichotomization}\label{sec:overview-dichotomization}
\textbf{Never} Dichotomize Continuous or Ordinal $Y$
  \bi
  \item Statistically optimum cutpoint is at the \textbf{unknown}
    population median  
    \bi
    \item power loss is still huge
    \ei
  \item If you cut at say 2 SDs from the population median, the loss
    of power can be massive, i.e., may have to increase sample size
    $\times 4$
  \item See Sections \ref{sec:info-catoutcomes} and \ref{sec:crohn}
  \item Avoid ``responder analysis''
  \item Serious ethical issues
  \item Dumbing-down $Y$ in the quest for clinical interpretability is
    a mistake.  Example:
    \bi
    \item Mean reduction in SBP 7mmHg $[2.5, 11.4]$ for B:A
    \item Proportion of pts achieving 10mmHg SBP reduction: A:0.31, B:0.41
      \bi
      \item Is the difference between 0.31 and 0.41 clinically significant?
      \item No information about reductions $> 10$ mmHg
      \ei
    \ei
  \item Can always restate optimum analysis results in other clinical metrics
  \ei
  
\subsection{Change from Baseline}\label{sec:overview-change}
\textbf{Never} use change from baseline as $Y$
  \bi
  \item Affected by measurement error, regression to the mean
  \item Assumes
    \bi
    \item you collected a second post-qualification baseline if the
      variable is part of inclusion/exclusion criteria
    \item variable perfectly transformed so that subtraction works
    \item post value linearly related to pre
    \item slope of pre on post is near 1.0
    \item no floor or ceiling effects
    \item $Y$ is interval-scaled
    \ei
  \item Appropriate analysis ($T$=treatment) \\
  $Y = \alpha + \beta_{1}\times T + \beta_{2} \times Y_{0}$ \\
  Easy to also allow nonlinear function of $Y_{0}$\\
  Also works well for ordinal $Y$ using a semiparametric model
  \item See Section \ref{sec:changegen} and Chapter \ref{chap:ancova}
  \ei

\section{Preprocessing}\alabel{sec:overview-preprocessing}
\bi
\item In vast majority of situations it is best to analyze the rawest
  form of the data
\item Pre-processing of data (e.g., normalization) is sometimes
  necessary when the data are high-dimensional
\item Otherwise normalizing factors should be part of the final
  analysis
\item A particularly bad practice in animal studies is to subtract or
  divide by measurements in a control group (or the experimental group
  at baseline), then to analyze the
  experimental group as if it is the only group.  Many things go
  wrong:
 \bi
 \item The normalization assumes that there is no biologic variability
   or measurement error in the control animals' measurements
 \item The data may have the property that it is inappropriate to
   either subtract or divide by other groups' measurements.  Division,
   subtraction, and percent change are highly parametric
   assumption-laden bases for analysis.
 \item A correlation between animals is induced by dividing by a
   random variable
\ei
\item A symptom of the problem is a graph in which the
  experimental group starts off with values 0.0 or 1.0
\item The only situation in which pre-analysis normalization is OK in
  small datasets is in pre-post design or certain crossover studies
  for which it is appropriate to subject baseline values from
  follow-up values
\ei
See also Section~\ref{sec:descript-change}.

\section{Random Variables}\label{sec:overview-rv}
\bi
\item A potential measurement $X$
\item $X$ might mean a blood pressure that will be measured on a
  randomly chosen US resident
\item Once the subject is chosen and the measurement is made, we have
  a sample value of this variable
\item Statistics often uses $X$ to denote a potentially observed value
  from some population and $x$ for an already-observed value (i.e., a
  constant)
\ei

\section{Probability}\label{sec:prob}
\movie{https://youtu.be/CBnGs9t6RxY}%
\movie{https://youtu.be/CDwZKyxk6Q4}%
\movie{https://youtu.be/GC-l345c1FY}%
\movie{https://youtu.be/cwADSMeiIoE}
As described below, the meaning of \emph{probability} is in the mind
of the beholder.  It can easily be taken to be a long-run relative
frequency, a degree of belief, or any metric that is between 0 and 1
that obeys certain basic rules (axioms) such as those of Kolmogorov:
\be
\item A probability is not negative.
\item The probability that at least one of the events in the exhaustive list of
  possible events occurs is 1.
\item The probability that at least one of a sequence of mutually
  exclusive events occurs equals the sum of the individual
  probabilities of the events occuring.
\ee

Let $A$ and $B$ denote events, or assertions about which we seek the
chances of their veracity.  The probabilities that $A$ or $B$ will
happen or are true are denoted by $P(A), P(B)$.

The above axioms lead to various useful properties, e.g.
\be
\item A probability cannot be greater than 1.
\item If $A$ is a special case of a more general
  event or assertion $B$, i.e., $A$ is a subset of $B$, $P(A) \leq
  P(B)$, e.g.\ $P($animal is human$) \leq P($animal is primate$)$.
\item $P(A \cup B)$, the probability of the union of $A$ and $B$,
  equals $P(A) + P(B) - P(A \cap B)$ where $A \cap B$ denotes the
  intersection (joint occurrence) of $A$ and $B$ (the overlap region).
\item If $A$ and $B$ are mutually exclusive, $P(A \cap B) = 0$ so $P(A
  \cup B) = P(A) + P(B)$.
\item $P(A \cup B) \geq \max(P(A), P(B))$
\item $P(A \cup B) \leq P(A) + P(B)$
\item $P(A \cap B) \leq \min(P(A), P(B))$
\item $P(A | B)$, the conditional probability of $A$ given $B$ holds,
  is $\frac{P(A \cap B)}{P(B)}$
\item $P(A \cap B) = P(A | B) P(B)$ whether or not $A$ and $B$ are
  independent.  If they are independent, $B$ is irrelevant to $P(A |
  B)$ so $P(A | B) = P(A)$, leading to the following statement:
\item If a set of events are independent, the probability of their
  intersection is the product of the individual probabilities.
\item The probability of the union of a set of events (i.e., the
  probability that at least one of the events occurs) is less than or
  equal to the sum of the individual event probabilities.
\item The probability of the intersection of a set of events (i.e.,
  the probability that all of the events occur) is less than or
  equal to the minimum of all the individual probabilities.
\ee

So what are examples of what probability might actually mean?
In the \emph{frequentist} school, the probability of
an event denotes the limit of the long-term fraction of occurrences
of the event.  This notion of probability implies that the same
experiment which generated the outcome of interest can be repeated
infinitely often\footnote{But even a coin will change after 100,000 flips.
Likewise, some may argue that a patient is ``one of a kind'' and that
repetitions of the same experiment are not possible.  One could
reasonably argue that a ``repetition'' does not denote the same
patient at the same stage of the disease, but rather \emph{any} patient
with the same \emph{severity} of disease (measured with current
technology).}.

There are other schools of probability that do not
require the notion of replication at all.  For example, the school of
\emph{subjective} probability (associated with the \emph{Bayesian}
school) ``considers probability as a measure of the degree of belief
of a given subject in the occurrence of an event or, more generally,
in the veracity of a given assertion'' (see P.\ 55 of~\cite{enc9}).
de~Finetti defined subjective probability in terms of wagers and odds
in betting.  A risk-neutral individual would be willing to wager \$$P$
that an event will occur when the payoff is \$1 and her subjective
probability is $P$ for the event.


As IJ Good has written, the axioms defining the ``rules'' under which probabilities
must operate (e.g., a probability is between 0 and 1) do not define
what a probability actually means.  He also summizes that all
probabilities are subjective, because they depend on the knowledge of
the particular observer.

One of the most important probability concepts is that of 
\emph{conditional probability}  The probability of the veracity of a statement 
or of an event $A$ occurring given that a specific condition $B$ holds or that an event 
$B$ has already occurred, is denoted by $P(A|B)$.  This is a probability in the 
presence of knowledge captured by $B$.  For example, if the condition $B$ is 
that a person is male, the conditional probability is the probability of $A$ 
for males, i.e., of males, what is the probability of $A$?.  It could
be argued that there is no such thing as a completely  
\emph{un}conditional probability.  In this example one is implicitly 
conditioning on humans even if not considering the person's sex.  Most
people would take $P($pregnancy$)$ to apply to females.

Conditional probabilities may be computed directly from restricted
subsets (e.g., males) or from this formula: $P(A|B)= \frac{P(A \cap
  B)}{P(B)}$.  That is, the probability that $A$ is true given $B$
occurred is the probability that both $A$ and $B$ happen (or are true)
divided by the probability of the conditioning event $B$.

\emph{Bayes' rule or theorem} is a ``conditioning reversal formula''
and follows from the basic probability laws:  $P(A | B) = \frac{P(B | A)
P(A)}{P(B)}$, read as the probability that event $A$ happens given
that event $B$ has happened equals the probability that $B$ happens
given that $A$ has happened multiplied by the (unconditional)
probability that $A$ happens and divided by the (unconditional)
probability that $B$ happens.  Bayes' rule follows immediately from
the law of conditional probability, which states that $P(A | B) =
\frac{P(A \cap B)}{P(B)}$.

The entire machinary of Bayesian inference derives from only Bayes'
theorem and the basic axioms of probability.  In contrast, frequentist
inference requires an enormous amount of extra machinary related to
the sample space, sufficient statistics, ancillary statistics, large
sample theory, and if taking more then one data look, stochastic
processes.  For many problems we still do not know how to 
accurately compute a frequentist $p$-value.

To understand conditional probabilities and Bayes' rule, consider the
probability that a randomly chosen U.S.\ senator is female.  As of
2017, this is $\frac{21}{100}$.  What is the probability that a
randomly chosen female in the U.S.\ is a U.S.\ senator?

\begin{eqnarray*}
P(\mathrm{senator}|\mathrm{female}) &=& \frac{P(\mathrm{female}|\mathrm{senator}) \times P(\mathrm{senator})}{P(\mathrm{female})} \\
 &=& \frac{\frac{21}{100} \times \frac{100}{326M}}{\frac{1}{2}} \\
 &=& \frac{21}{163M}
 \end{eqnarray*}

So given the marginal proportions of senators and females, we can use
Bayes' rule to convert ``of senators how many are females'' to ``of
females how many are senators.''

The domain of application of
probability is all-important.  We assume that the true event status
(e.g., dead/alive) is unknown, and we also assume that the information
the probability is conditional upon (e.g.  $P(\mathrm{death}|\mathrm{male,
age=70})$ is what we would check the probability against.  In other
words, we do not ask whether $P(\mathrm{death} | \mathrm{male,
  age=70})$ is accurate 
when compared against $P(\mathrm{death} |$ male, age=70, meanbp=45, patient
on downhill course$)$.  It is difficult to find a probability that is
truly not conditional on anything.  What is conditioned upon is all
important.  Probabilities are maximally useful when, as with Bayesian
inference, they condition on what is known to provide a forecast for
what is unknown.  These are ``forward time'' or ``forward information
flow'' probabilities.

Forward time probabilities can meaningfully be
taken out of context more often than backward-time probabilities, as
they don't need to consider ``what might have happened.''  In
frequentist statistics, the $P$-value is a backward information flow
probability, being conditional on the unknown effect size.  This is
why $P$-values must be adjusted for multiple data looks (what might
have happened, i.e., what data might have been observed were $H_{0}$
true) whereas the current Bayesian posterior probability 
merely override any posterior probabilities computed at earlier data
looks, because they condition on current cumulative data.
