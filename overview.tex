\chapter{General Overview of Biostatistics} \katz{1-3}  \ems{2} \abd{1.1,p.23-4}\label{chap:overview}
\bmovie{1}\ddisc{1}

\quoteit{There are no routine statistical questions, only questionable
  statistical routines.}{Sir David R.\ Cox}

\quoteit{It's much easier to get a \emph{result} than it is to get an
  \emph{answer}.}{Christie Aschwanden, \texttt{FireThirtyEight}}

\section{What is Biostatistics?}\label{sec:overview-biostat}
\bi
\item Statistics applied to biomedical problems
\item Decision making in the face of uncertainty or variability
\item Design and analysis of experiments; detective work in
  observational studies (in epidemiology, outcomes research, etc.)
\item Attempt to remove bias or find alternative explanations to those
  posited by researchers with vested interests
\item Experimental design, measurement, description, statistical graphics,
  data analysis, inference, prediction
  \ei

To optimize its value, biostatistics needs to be fully integrated into
biomedical research and we must recognize that experimental design and
execution (e.g., randomization and masking) are all important.

\subsection{Branches of Statistics}
\bi
\item Frequentist (traditional)
\item Bayesian
\item Likelihoodist (a bit like Bayes without priors)
\ei

See Section~\ref{sec:htest-branches}.


\subsection{Fundamental Principles of Statistics}
\bi
\item Use methods grounded in theory or extensive simulation
\item Understand uncertainty
\item Design experiments to maximize information and understand
  sources of variability
\item Use all information in data during analysis
\item Use discovery and estimation procedures not likely to claim that
  noise is signal
\item Strive for optimal quantification of evidence about effects
\item Give decision makers the inputs (\emph{other} than the utility
  function\footnote{The utility function is also called the loss or
    cost function.  It specifies, for example, the damage done by
    making various decisions such as treating patients who don't have
    the disease or failing to treat those who do.  The optimum Bayes
    decision is the one that minimizes expected loss.  This decision
    conditions on full information and uses for example predicted risk
    rather than whether or not the predicted risk is high.}) that
  optimize decisions
  \bi
   \item Not directly actionable: probabilities that condition on the
     future to predict the past/present, i.e, those conditioning on
     the unknown
     \bi
     \item sensitivity and specificity ($P(\textrm{test result} |
       \textrm{disease status})$)\\
       Sensitivity irrelevant once it is known that the test is +
     \item $p$-values (condition on effect being zero)
     \ei
  \ei
\item Present information in ways that are intuitive, maximize
  information content, and are correctly perceived
\ei

\section{What Can Statistics Do?}
\bi
\item Refine measurements
\item Experimental design
  \bi
  \item Make sure design answers the question
  \item Take into account sources of variability
  \item Identify sources of bias
  \item Developing sequential or adaptive designs
  \item Avoid wasting subjects
    \ei
\item {\smaller (in strong collaboration with epidemiologists)} Observational
  study design
\item {\smaller (in strong collaboration with epidemiologists and
  philosophers)} Causal inference
\item Use methods that preserve all relevant information in data
\item Robust analysis optimizing power, minimizing assumptions
\item Estimating magnitude of effects
\item Estimating \textbf{shapes} of effects of continuous predictors
\item Quantifying causal evidence for effects if the design is appropriate
\item Adjusting for confounders
\item Properly model effect modification (interaction) / heterogeneity
  of treatment effect
\item Developing and validating predictive models
\item Choosing optimum measures of predictive accuracy
\item Quantify information added by new measurements / medical tests
\item Handling missing data or measurements below detection limits
\item Risk-adjusted scorecards (e.g., health provider profiling)
\item Visual presentation of results taken into account graphical
  perception
\item Finding alternate explanations for observed phenomena
\item Foster reproducible research
\ei   

See
\href{http://biostat.mc.vanderbilt.edu/BenefitsBasicSci}{biostat.mc.vanderbilt.edu/BenefitsBasicSci}
for more benefits of biostatistics.

\subsection{Statistical Scientific Method}
\bi
\item Statistics is not a bag of tools and math formulas but an
  evidence-based way of thinking 
\item It is all important to
  \bi
   \item understand the problem
   \item properly frame the question to address it
   \item understand and optimize the measurements
   \item understand sources of variability
   \item much more
     \ei
\item MacKay \& Oldford~\cite{mac00sci} developed a 5-stage
  representation of the statistical method applied to scientific
  investigation: \textbf{\emph{Problem, Plan, Data, Analysis,
      Conclusion}} having the elements below:
  \ei

{\tszs
  \begin{center}
    \begin{tabular}{ll} \hline
    \textbf{Problem} & Units \& Target Population (Process)\\
    & Response Variate(s)\\
    & Explanatory Variates\\
    & Population Attribute(s)\\
    & Problem Aspect(s) â€“ causative, descriptive, predictive\\
    & \\
    \textbf{Plan} & Study Population (Process)\\
    & ~~~(Units, Variates, Attributes)\\
    & Selecting the response variate(s)\\
    & Dealing with explanatory variates\\
    & Sampling Protocol\\
    & Measuring process\\
    & Data Collection Protocol\\
    & \\
\textbf{Data} & Execute the Plan\\
    & ~~~and record all departures\\
    & Data Monitoring\\
    & Data Examination\\
    & ~~~for internal consistency\\
    & Data storage\\
    & \\
\textbf{Analysis} & Data Summary\\
    & ~~~numerical and graphical\\
    & Model construction\\
    & ~~~build, fit, criticize cycle\\
    & Formal analysis\\
    & \\
\textbf{Conclusion} & Synthesis\\
    & ~~~plain language, effective presentation graphics\\
    & Limitations of study\\
    & ~~~discussion of potential errors\\ \hline
\end{tabular}
\end{center}
}

\textbf{Recommended Reading for Experimental Design}

Glass~\cite{gla14exp}, Ruxton and Colegrave~\cite{rux17exp}, and
Chang~\cite{cha16pri}.

\textbf{Recommended Reading for Clinical Study Design}

Hulley~\etal~\cite{hul13des}.

\subsubsection{Pointers for Observational Study Design}   %NEW SINCE VIDEO
\bi
\item Understand the problem and formulate a pertinent question
\item Figure out and be able to defend observaton periods and ``time zero''
\item Carefully define subject inclusion/exclusion criteria
\item Determine which measurements are required for answering the
  question while accounting for alternative explanations.  Do this
  \textbf{before} examining existing datasets so as to not engage in
  rationalization bias.
\item Collect these measurements or verify that an already existing
  dataset contains all of them
\item Make sure that the measurements are not missing too often and
  that measurement error is under control.  This is even slightly more
  important for inclusion/exclusion criteria.
\item Make sure the use of observational data respects causal
  pathways.  For example don't use outcome/response/late-developing
  medical complications as if they were independent variables.
\ei
  
\section{Types of Data Analysis and Inference}\label{sec:overview-datatypes}
 \bi
 \item Description: what happened to \emph{past} patients
 \item Inference from specific (a sample) to general (a population)
 \bi
  \item Hypothesis testing: test a hypothesis about population or
   long-run effects
  \item Estimation: approximate a population or long term average
    quantity
    \ei
  \item Bayesian inference
    \bi
    \item Data may not be a sample from a population
    \item May be impossible to obtain another sample
    \item Seeks knowledge of hidden process generating \textbf{this}
      sample (generalization of inference to population)
    \ei
 \item Prediction: predict the responses of other patients \emph{like yours}
   based on analysis of patterns of responses in your patients
   \ei

Leek and Peng~\cite{lee15wha} created a nice data analysis flowchart.

\begin{center}\includegraphics[width=.8\textwidth]{lee15wha.png}\end{center}

They also have a succinct summary of common statistical mistakes
originating from a failure to match the question with the analysis.

\begin{center}\includegraphics[width=.6\textwidth]{lee15wha2.png}\end{center}

\section{Types of Measurements by Their Role in the Study} \katz{3} \abd{1.3}\label{sec:overview-measurement-role}
\bi
 \item Response variable (clinical endpoint, final lab measurements,
   etc.)
 \item Independent variable (predictor or descriptor variable) ---
   something measured when a patient begins to be studied, before the
   response; often not controllable by 
   investigator, e.g. sex, weight, height, smoking history
 \item Adjustment variable (confounder) --- a variable not of major
   interest but one needing accounting for because it explains an
   apparent effect of a variable of major interest or because it
   describes heterogeneity in severity of risk factors across patients
 \item Experimental variable, e.g. the treatment or dose to which a
   patient is randomized; this is an independent variable under the
   control of the researcher
 \ei

\begin{table}[h!]
 \caption{Common alternatives for describing independent and response variables}
\begin{center}
\begin{tabular}{ll} \hline \hline
Response variable & Independent variable \\ \hline
Outcome variable & Exposure variable \\
Dependent variable & Predictor variable \\
$y$-variables &  $x$-variable \\
Case-control group &  Risk factor \\
 & Explanatory variable \\ \hline \hline
\end{tabular}
\end{center}
\end{table}

\subsection{Proper Response Variables}\label{overview-yproper}
It is too often the case that researchers concoct response variables $Y$
in such a way that makes the variables \emph{seem} to be easy to
interpret, but which contain several hidden problems:
\bi
\item $Y$ may be a categorization/dichotomization of an underlying
  continuous response variable.  The cutpoint used for the
  dichotomization is never consistent with data (see
  Figure~\ref{fig:info-thresholds}), 
  is arbitrary (P.~\pageref{pg:info-gia14opt}), and causes a huge loss of
  statistical information and power (P.~\pageref{pg:info-fed09con}).
\item $Y$ may be based on a change in a subject's condition whereas
  what is truly important is the subject's most recent condition
  (P.~\pageref{pg:change-anova}).
\item $Y$ may be based on change when the underlying variable is not
  monotonically related to the ultimate outcome, indicating that
  positive change is good for some subjects and bad for others
  (Fig.~\ref{fig:change-suppcr}).
\ei
A proper response variable that optimizes power is one that
\bi
\item Captures the underlying structure or process
\item Has low measurement error
\item Has the highest resolution available, e.g.
 \bi
 \item is continuous if the underlying measurement is continuous
 \item is ordinal with several categories if the underlying
   measurement is ordinal
 \item is binary only if the underlying process is truly
   all-or-nothing
 \ei
\item Has the same interpretation for every type of subject, and
  especially has a direction such that higher values are always good
  or always bad
\ei

\section{Types of Measurements According to Coding} \katz{3} \ems{2.2}\abd{1.3}\label{sec:overview-coding}
\bi
 \item Binary: yes/no, present/absent
 \item Categorical (aka nominal, polytomous, discrete, multinomial):
   more than 2 values that are not necessarily in special order
 \item Ordinal: a categorical variable whose possible values are in a
   special order, e.g., by severity of symptom or disease; spacing
   between categories is not assumed to be useful
  \bi
  \item Ordinal variables that are not continuous often have heavy
    ties at one or more values requiring the use of statistical
    methods that allow for strange distributions and handle ties well
  \item Continuous are also ordinal but ordinal variables may or may
    not be continuous
  \ei
 \item Count: a discrete variable that (in theory) has no upper limit, e.g. the number of ER visits in a day, the number of traffic accidents in a month
 \item Continuous: a numeric variable having many possible values
   representing an underlying spectrum
 \item Continuous variables have the most statistical information
  (assuming the raw values are used in the data analysis) and
  are usually the easiest to standardize across hospitals
 \item Turning continuous variables
  into categories by using intervals of values is arbitrary and
  requires more patients to yield the same statistical information
  (precision or power)
 \item Errors are not reduced by categorization unless that's the only
   way to get a subject to answer the question (e.g.,
 income\footnote{But note how the Census Bureau tries to maximize the
 information collected.  They first ask for income in
 dollars.  Subjects refusing to answer are asked to choose from among
 10 or 20 categories.  Those not checking a category are asked to
 choose from fewer categories.})
\ei

\section{Choose $Y$ to Maximize Statistical Information, Power, and Interpretability}\label{sec:overview-ychoice}
The outcome (dependent) variable $Y$ should be a high-information \blog{ordinal-info}
measurement that is relevant to the subject at hand.  The information
provided by an analysis, and statistical power and precision, are
strongly influenced by characteristics of $Y$ in addition to the
effective sample size.
  \bi
  \item Noisy $Y \rightarrow$ variance $\uparrow$, effect of
    interest $\downarrow$
  \item Low information content/resolution also $\rightarrow$ power
    $\downarrow$ 
  \item Minimum information $Y$: binary outcome
  \item Maximum information $Y$: continuous response with almost no
    measurement error
    \bi
    \item Example: measure systolic blood pressure (SBP) well and
      average 5 readings 
    \ei
  \item Intermediate: ordinal $Y$ with a few well-populated levels
  \item Exploration of power vs.\ number of ordinal $Y$ levels and
    degree of balance in frequencies of levels: \href{https://fharrell.com/post/ordinal-info}{fharrell.com/post/ordinal-info}
  \item See Section \ref{sec:htest-mult} for examples of ordinal outcome scales and interpretation of results
  % Above not on video
  \ei

\subsection{Information Content}\label{sec:overview-info-content}
  \bi
  \item Binary $Y$: 1 bit
    \bi
    \item all--or--nothing
    \item no gray zone, close calls
    \item often arbitrary
    \ei
  \item SBP: $\approx$ 5 bits
    \bi
    \item range 50-250mmHg (7 bits)
    \item accurate to nearest 4mmHg (2 bits)
    \ei
  \item Time to binary event: if proportion of subjects having event is
    small, is effectively a binary endpoint
    \bi
    \item becomes truly continuous and yields high power if proportion
      with events much greater than $\frac{1}{2}$, if time to event is
      clinically meaningful
    \item if there are multiple events, or you pool events of
      different severities, time to first event loses information
    \ei
  \ei
  
\subsection{Dichotomization}\label{sec:overview-dichotomization}
\textbf{Never} Dichotomize Continuous or Ordinal $Y$
  \bi
  \item Statistically optimum cutpoint is at the \textbf{unknown}
    population median  
    \bi
    \item power loss is still huge
    \ei
  \item If you cut at say 2 SDs from the population median, the loss
    of power can be massive, i.e., may have to increase sample size
    $\times 4$
  \item See Sections \ref{sec:info-catoutcomes} and \ref{sec:crohn}
  \item Avoid ``responder analysis'' (see \href{http://datamethods.org/t/responder-analysis-loser-x-4}{datamethods.org/t/responder-analysis-loser-x-4})
  \item Serious ethical issues
  \item Dumbing-down $Y$ in the quest for clinical interpretability is
    a mistake.  Example:
    \bi
    \item Mean reduction in SBP 7mmHg $[2.5, 11.4]$ for B:A
    \item Proportion of pts achieving 10mmHg SBP reduction: A:0.31, B:0.41
      \bi
      \item Is the difference between 0.31 and 0.41 clinically significant?
      \item No information about reductions $> 10$ mmHg
      \ei
    \ei
  \item Can always restate optimum analysis results in other clinical metrics
  \ei
  
\subsection{Change from Baseline}\label{sec:overview-change}
\textbf{Never} use change from baseline as $Y$
  \bi
  \item Affected by measurement error, regression to the mean
  \item Assumes
    \bi
    \item you collected a second post-qualification baseline if the
      variable is part of inclusion/exclusion criteria
    \item variable perfectly transformed so that subtraction works
    \item post value linearly related to pre
    \item slope of pre on post is near 1.0
    \item no floor or ceiling effects
    \item $Y$ is interval-scaled
    \ei
  \item Appropriate analysis ($T$=treatment) \\
  $Y = \alpha + \beta_{1}\times T + \beta_{2} \times Y_{0}$ \\
  Easy to also allow nonlinear function of $Y_{0}$\\
  Also works well for ordinal $Y$ using a semiparametric model
  \item See Section \ref{sec:changegen} and Chapter \ref{chap:ancova}
  \ei

\section{Preprocessing}\alabel{sec:overview-preprocessing}
\bi
\item In vast majority of situations it is best to analyze the rawest
  form of the data
\item Pre-processing of data (e.g., normalization) is sometimes
  necessary when the data are high-dimensional
\item Otherwise normalizing factors should be part of the final
  analysis
\item A particularly bad practice in animal studies is to subtract or
  divide by measurements in a control group (or the experimental group
  at baseline), then to analyze the
  experimental group as if it is the only group.  Many things go
  wrong:
 \bi
 \item The normalization assumes that there is no biologic variability
   or measurement error in the control animals' measurements
 \item The data may have the property that it is inappropriate to
   either subtract or divide by other groups' measurements.  Division,
   subtraction, and percent change are highly parametric
   assumption-laden bases for analysis.
 \item A correlation between animals is induced by dividing by a
   random variable
\ei
\item A symptom of the problem is a graph in which the
  experimental group starts off with values 0.0 or 1.0
\item The only situation in which pre-analysis normalization is OK in
  small datasets is in pre-post design or certain crossover studies
  for which it is appropriate to subject baseline values from
  follow-up values
\ei
See also Section~\ref{sec:descript-change}.

\section{Random Variables}\label{sec:overview-rv}\bmovie{2}\ddisc{2}
\bi
\item A potential measurement $X$
\item $X$ might mean a blood pressure that will be measured on a
  randomly chosen US resident
\item Once the subject is chosen and the measurement is made, we have
  a sample value of this variable
\item Statistics often uses $X$ to denote a potentially observed value
  from some population and $x$ for an already-observed value (i.e., a
  constant)
  \ei

\textbf{But} think about the clearer terminology of
\href{https://youtu.be/yakg94HyWdE?t=2890}{Richard McElreath}\footnote{\url{https://youtu.be/yakg94HyWdE?t=2890}}:

\begin{center}
  \begin{tabular}{cc}
    Convention & Proposal \\ \hline
    Data & Observed variable \\
    Parameter & Unobserved variable \\
    Likelihood & Distribution \\
    Prior & Distribution \\
    Posterior & Conditional distribution \\
    Estimate & \emph{banished} \\
    Random & \emph{banished}
  \end{tabular}
  \end{center}

\section{Probability}\label{sec:prob}
\movie{https://youtu.be/CBnGs9t6RxY}%
\movie{https://youtu.be/CDwZKyxk6Q4}%
\movie{https://youtu.be/GC-l345c1FY}%
\movie{https://youtu.be/cwADSMeiIoE}

\bi
\item \emph{Probability} traditionally taken as long-run relative
  frequency
\item Example: batting average of a baseball player (long-term
  proportion of at-bat opportunities resulting in a hit)
\item \textbf{Not so fast}: The batting average
  \bi
   \item depends on pitcher faced
   \item may drop over a season as player tires or is injured
   \item drops over years as the player ages
   \ei
\item Getting a hit may be better thought of as a one-time event for
  which batting average is an approximation of the probability
\ei
     
As described below, the meaning of \emph{probability} is in the mind
of the beholder.  It can easily be taken to be a long-run relative
frequency, a degree of belief, or any metric that is between 0 and 1
that obeys certain basic rules (axioms) such as those of Kolmogorov:
\be
\item A probability is not negative.
\item The probability that at least one of the events in the exhaustive list of
  possible events occurs is 1.
  \bi
  \item Example: possible events death, nonfatal myocardial infarction
    (heart attack), or neither
  \item P(at least one of these occurring) = 1
  \ei
\item The probability that at least one of a sequence of mutually
  exclusive events occurs equals the sum of the individual
  probabilities of the events occurring.
  \bi
  \item P(death or nonfatal MI) = P(death) + P(nonfatal MI)
  \ei
\ee

Let $A$ and $B$ denote events, or assertions about which we seek the
chances of their veracity.  The probabilities that $A$ or $B$ will
happen or are true are denoted by $P(A), P(B)$.

The above axioms lead to various useful properties, e.g.
\be
\item A probability cannot be greater than 1.
\item If $A$ is a special case of a more general
  event or assertion $B$, i.e., $A$ is a subset of $B$, $P(A) \leq
  P(B)$, e.g.\ $P($animal is human$) \leq P($animal is primate$)$.
\item $P(A \cup B)$, the probability of the union of $A$ and $B$,
  equals $P(A) + P(B) - P(A \cap B)$ where $A \cap B$ denotes the
  intersection (joint occurrence) of $A$ and $B$ (the overlap region).
\item If $A$ and $B$ are mutually exclusive, $P(A \cap B) = 0$ so $P(A
  \cup B) = P(A) + P(B)$.
\item $P(A \cup B) \geq \max(P(A), P(B))$
\item $P(A \cup B) \leq P(A) + P(B)$
\item $P(A \cap B) \leq \min(P(A), P(B))$
\item $P(A | B)$, the conditional probability of $A$ given $B$ holds,
  is $\frac{P(A \cap B)}{P(B)}$
\item $P(A \cap B) = P(A | B) P(B)$ whether or not $A$ and $B$ are
  independent.  If they are independent, $B$ is irrelevant to $P(A |
  B)$ so $P(A | B) = P(A)$, leading to the following statement:
\item If a set of events are independent, the probability of their
  intersection is the product of the individual probabilities.
\item The probability of the union of a set of events (i.e., the
  probability that at least one of the events occurs) is less than or
  equal to the sum of the individual event probabilities.
\item The probability of the intersection of a set of events (i.e.,
  the probability that all of the events occur) is less than or
  equal to the minimum of all the individual probabilities.
\ee

So what are examples of what probability might actually mean?
In the \emph{frequentist} school, the probability of
an event denotes the limit of the long-term fraction of occurrences
of the event.  This notion of probability implies that the same
experiment which generated the outcome of interest can be repeated
infinitely often\footnote{But even a coin will change after 100,000 flips.
Likewise, some may argue that a patient is ``one of a kind'' and that
repetitions of the same experiment are not possible.  One could
reasonably argue that a ``repetition'' does not denote the same
patient at the same stage of the disease, but rather \emph{any} patient
with the same \emph{severity} of disease (measured with current
technology).}.

There are other schools of probability that do not
require the notion of replication at all.  For example, the school of
\emph{subjective} probability (associated with the \emph{Bayesian}
school) ``considers probability as a measure of the degree of belief
of a given subject in the occurrence of an event or, more generally,
in the veracity of a given assertion'' (see P.\ 55 of~\cite{enc9}).
de~Finetti defined subjective probability in terms of wagers and odds
in betting.  A risk-neutral individual would be willing to wager \$$P$
that an event will occur when the payoff is \$1 and her subjective
probability is $P$ for the event.


As IJ Good has written, the axioms defining the ``rules'' under which probabilities
must operate (e.g., a probability is between 0 and 1) do not define
what a probability actually means.  He also surmises that all
probabilities are subjective, because they depend on the knowledge of
the particular observer.

One of the most important probability concepts is that of 
\emph{conditional probability}  The probability of the veracity of a statement 
or of an event $A$ occurring given that a specific condition $B$ holds or that an event 
$B$ has already occurred, is denoted by $P(A|B)$.  This is a probability in the 
presence of knowledge captured by $B$.  For example, if the condition $B$ is 
that a person is male, the conditional probability is the probability of $A$ 
for males, i.e., of males, what is the probability of $A$?.  It could
be argued that there is no such thing as a completely  
\emph{un}conditional probability.  In this example one is implicitly 
conditioning on humans even if not considering the person's sex.  Most
people would take $P($pregnancy$)$ to apply to females.

Conditional probabilities may be computed directly from restricted
subsets (e.g., males) or from this formula: $P(A|B)= \frac{P(A \cap
  B)}{P(B)}$.  That is, the probability that $A$ is true given $B$
occurred is the probability that both $A$ and $B$ happen (or are true)
divided by the probability of the conditioning event $B$.

\emph{Bayes' rule or theorem} is a ``conditioning reversal formula''
and follows from the basic probability laws:  $P(A | B) = \frac{P(B | A)
P(A)}{P(B)}$, read as the probability that event $A$ happens given
that event $B$ has happened equals the probability that $B$ happens
given that $A$ has happened multiplied by the (unconditional)
probability that $A$ happens and divided by the (unconditional)
probability that $B$ happens.  Bayes' rule follows immediately from
the law of conditional probability, which states that $P(A | B) =
\frac{P(A \cap B)}{P(B)}$.

The entire machinery of Bayesian inference derives from only Bayes'
theorem and the basic axioms of probability.  In contrast, frequentist
inference requires an enormous amount of extra machinery related to
the sample space, sufficient statistics, ancillary statistics, large
sample theory, and if taking more then one data look, stochastic
processes.  For many problems we still do not know how to 
accurately compute a frequentist $p$-value.

To understand conditional probabilities and Bayes' rule, consider the
probability that a randomly chosen U.S.\ senator is female.  As of
2017, this is $\frac{21}{100}$.  What is the probability that a
randomly chosen female in the U.S.\ is a U.S.\ senator?

\begin{eqnarray*}
P(\mathrm{senator}|\mathrm{female}) &=& \frac{P(\mathrm{female}|\mathrm{senator}) \times P(\mathrm{senator})}{P(\mathrm{female})} \\
 &=& \frac{\frac{21}{100} \times \frac{100}{326M}}{\frac{1}{2}} \\
 &=& \frac{21}{163M}
 \end{eqnarray*}

So given the marginal proportions of senators and females, we can use
Bayes' rule to convert ``of senators how many are females'' to ``of
females how many are senators.''

The domain of application of
probability is all-important.  We assume that the true event status
(e.g., dead/alive) is unknown, and we also assume that the information
the probability is conditional upon (e.g.  $P(\mathrm{death}|\mathrm{male,
age=70})$ is what we would check the probability against.  In other
words, we do not ask whether $P(\mathrm{death} | \mathrm{male,
  age=70})$ is accurate 
when compared against $P(\mathrm{death} |$ male, age=70, meanbp=45, patient
on downhill course$)$.  It is difficult to find a probability that is
truly not conditional on anything.  What is conditioned upon is all
important.  Probabilities are maximally useful when, as with Bayesian
inference, they condition on what is known to provide a forecast for
what is unknown.  These are ``forward time'' or ``forward information
flow'' probabilities.

Forward time probabilities can meaningfully be
taken out of context more often than backward-time probabilities, as
they don't need to consider ``what might have happened.''  In
frequentist statistics, the $P$-value is a backward information flow
probability, being conditional on the unknown effect size.  This is
why $P$-values must be adjusted for multiple data looks (what might
have happened, i.e., what data might have been observed were $H_{0}$
true) whereas the current Bayesian posterior probability 
merely overrides any posterior probabilities computed at earlier data
looks, because they condition on current cumulative data.
