dim(w)
w$mean
w['mean']
dim(w)
w
w['mean']
# Use an arbitrary large sample to mimic population computations
m   <- 200000
y1  <- sort(rnorm(m, 100, 10))
ors <- means <- seq(1, 4, by=.01)
i   <- 0
for(or in ors) {
i <- i + 1
means[i] <- pomodm(y1, rep(1/m, m), odds.ratio=or)['mean']
}
# Use an arbitrary large sample to mimic population computations
m   <- 200000
y1  <- sort(rnorm(m, 100, 10))
ors <- means <- seq(1, 4, by=.01)
i   <- 0
for(or in ors) {
i <- i + 1
means[i] <- pomodm(y1, rep(1/m, m), odds.ratio=or)['mean']
}
plot(ors, means)
plot(ors, means, xlab='OR', ylab='Mean', type='l')
abline(h=103, color=gray(.85))
abline(h=103, col=gray(.85))
approx(means, ors, xout=103)$y
needed.or <- approx(means, ors, xout=103)$y
needed.or
abline(v=needed.or, col=gray(.85))
# Compute power at that odds ratio assuming no ties in data
power(rep(1/300, 300), odds.ratio=needed.or, n=300)
# Compute power at that odds ratio assuming no ties in data
popower(rep(1/300, 300), odds.ratio=needed.or, n=300)
plot(ors, means, xlab='Group B:A Odds Ratio',
ylab='Mean in Population B', type='l')
abline(h=103, col=gray(.85))
needed.or <- approx(means, ors, xout=103)$y
needed.or
abline(v=needed.or, col=gray(.85))
# Compute power at that odds ratio assuming no ties in data
popower(rep(1/300, 300), odds.ratio=needed.or, n=300)
?rt
t <- t.test(y1, y2)
names(t)
?t.test
s <- 1000    # number of simulated trials
pvalt <- pvalw <- numeric(s)
set.seed(1)  # so can reproduce results
for(i in 1 : 1000) {
y1 <- rt(150, 4)
y2 <- rt(150, 4) + 0.3
pvalt[i] <- t.test(y1, y2)$p.value
pvalw[i] <- wilcox.test(y1, y2)$p.value
}
mean(pvalt); mean(pvalw)
mean(pvalt < 0.05)   # proportion of simulations with p < 0.05
mean(pvalw < 0.05)
mean(pvalw < pvalt)  # proportino of simulations with smaller p for W
mean(pvalt < 0.05)   # proportion of simulations with p < 0.05
mean(pvalw < 0.05)
s <- 1000    # number of simulated trials
pvalt <- pvalw <- numeric(s)
set.seed(1)  # so can reproduce results
for(type in c('null', 'alt')) {
for(i in 1 : s) {
y1 <- rt(150, 4)
y2 <- rt(150, 4) + 0.3
pvalt[i] <- t.test(y1, y2)$p.value
pvalw[i] <- wilcox.test(y1, y2)$p.value
}
cat('Case:', type, '\n')
r <- function(x) round(mean(x), 2)
cat('Proportion of simulations with W p-value < t p-value:',
r(pvalw < pvalt), '\n')
cat('Mean p-value for t:', r(pvalt), '\n')
cat('Mean p-value for W:', r(pvalw), '\n')
cat('Power for t:', m(pvalt < 0.05), '\n')
cat('Power for W:', m(pvalw < 0.05), '\n')
}
for(type in c('null', 'alt')) {
for(i in 1 : s) {
y1 <- rt(150, 4)
y2 <- rt(150, 4) + 0.3
pvalt[i] <- t.test(y1, y2)$p.value
pvalw[i] <- wilcox.test(y1, y2)$p.value
}
cat('Case:', type, '\n')
r <- function(x) round(mean(x), 2)
cat('Proportion of simulations with W p-value < t p-value:',
r(pvalw < pvalt), '\n')
cat('Mean p-value for t:', r(pvalt), '\n')
cat('Mean p-value for W:', r(pvalw), '\n')
cat('Power for t:', r(pvalt < 0.05), '\n')
cat('Power for W:', r(pvalw < 0.05), '\n')
}
pvalt <- pvalw <- numeric(s)
set.seed(1)  # so can reproduce results
for(type in c('null', 'alt')) {
for(i in 1 : s) {
y1 <- rt(150, 4)
y2 <- rt(150, 4) + ifelse(type == 'null', 0, 0.3)
pvalt[i] <- t.test(y1, y2)$p.value
pvalw[i] <- wilcox.test(y1, y2)$p.value
}
cat('Case:', type, '\n')
r <- function(x) round(mean(x), 2)
cat('Proportion of simulations with W p-value < t p-value:',
r(pvalw < pvalt), '\n')
cat('Mean p-value for t:', r(pvalt), '\n')
cat('Mean p-value for W:', r(pvalw), '\n')
cat('Power for t:', r(pvalt < 0.05), '\n')
cat('Power for W:', r(pvalw < 0.05), '\n')
}
s <- 1000    # number of simulated trials
pvalt <- pvalw <- numeric(s)
set.seed(1)  # so can reproduce results
for(type in c('null', 'alt')) {
for(i in 1 : s) {
y1 <- rt(150, 4)
y2 <- rt(150, 4) + ifelse(type == 'null', 0, 0.3)
pvalt[i] <- t.test(y1, y2)$p.value
pvalw[i] <- wilcox.test(y1, y2)$p.value
}
cat('Case:', type, '\n')
r <- function(x) round(mean(x), 2)
cat('Proportion of simulations with W p-value < t p-value:',
r(pvalw < pvalt), '\n')
cat('Mean p-value for t:', r(pvalt), '\n')
cat('Mean p-value for W:', r(pvalw), '\n')
cat('Power for t:', r(pvalt < 0.05), '\n')
cat('Power for W:', r(pvalw < 0.05), '\n')
}
?sample
?pomodm
require(Hmisc)
?pomodm
# Use an arbitrary large sample to mimic population computations
m   <- 200000
y1  <- sort(rnorm(m, 100, 10))
# Control arm has Gaussian Y
# Compute a new discrete distribution if we convert the control
# distribution using proportional odds
# Done by using a discrete distribution with 200,000 points
p <- pomodm(p=rep(1/m, m), odds.ratio=1)
length(p)
table(p)
range(p)
1/m
# Control arm has Gaussian Y
# Compute a new discrete distribution if we convert the control
# distribution using proportional odds
# Done by using a discrete distribution with 200,000 points
p <- pomodm(p=rep(1/m, m), odds.ratio=5)
hist(p, nclass=200)
range(p)
args(wtd.mean)
wtd.mean(y1, p)
?sample
# Sample from the shifted distribution
y2 <- sample(y1, 50000, prob=p)
mean(y2)
length(y2)
?qqnorm
qqnorm(y2)
qqline(y2)
hist(y2, nclass=100)
qqnorm(y2)
hist(y2, nclass=100, main='')     # heavier left tail
qqnorm(y2)
qqnorm(y2, type='l')
qqnorm(y2, type='l')
Ecdf(y2)
length(y2)
args(pnorm)
y <- seq(60, 145, length=150)
lines(y, pnorm(y, mean=mean(y2), sd=sd(y2)), col='blue')
# Control arm has Gaussian Y
# Compute a new discrete distribution if we convert the control
# distribution using proportional odds
# Done by using a discrete distribution with 200,000 points
p <- pomodm(p=rep(1/m, m), odds.ratio=10)
range(p)  # control arm: all 1/200000
wtd.mean(y1, p)  # mean shifted by about 9 units
# Sample from the shifted distribution
y2 <- sample(y1, 50000, prob=p)
mean(y2)
# Control arm has Gaussian Y
# Compute a new discrete distribution if we convert the control
# distribution using proportional odds
# Done by using a discrete distribution with 200,000 points
p <- pomodm(p=rep(1/m, m), odds.ratio=10)
range(p)  # control arm: all 1/200000
wtd.mean(y1, p)  # mean shifted by about 12 units
# Sample from the shifted distribution
y2 <- sample(y1, 50000, prob=p)
mean(y2)
# Control arm has Gaussian Y
# Compute a new discrete distribution if we convert the control
# distribution using proportional odds
# Done by using a discrete distribution with 200,000 points
p <- pomodm(p=rep(1/m, m), odds.ratio=10)
range(p)  # control arm: all 1/200000
wtd.mean(y1, p)  # mean shifted by about 12 units
# Sample from the shifted distribution
y2 <- sample(y1, 50000, prob=p)
mean(y2)
hist(y2, nclass=100, main='')     # heavier left tail
Ecdf(y2)
y <- seq(60, 145, length=150)
lines(y, pnorm(y, mean=mean(y2), sd=sd(y2)), col='blue')
# Use an arbitrary large sample to mimic population computations
m   <- 200000
y1  <- sort(rnorm(m, 100, 10))
mean(y1)
wtd.mean(y1, rep(1/m, m))
# Control arm has Gaussian Y
# Compute a new discrete distribution if we convert the control
# distribution using proportional odds
# Done by using a discrete distribution with 200,000 points
p <- pomodm(p=rep(1/m, m), odds.ratio=10)
range(p)  # control arm: all 1/200000
?wtd.mean
wtd.mean(y1, p)  # mean shifted by about 12 units
?sample
# Sample from the shifted distribution
y2 <- sample(y1, 50000, prob=p)
mean(y2)
hist(y2, nclass=100, main='')     # heavier left tail
Ecdf(y2)
y <- seq(60, 145, length=150)
lines(y, pnorm(y, mean=mean(y2), sd=sd(y2)), col='blue')
# First do this theoretically
# Control arm has Gaussian Y with mean 100, SD 10
# Create experimenetal arm distribution using OR=10
y <- seq(60, 145, length=150)
Fy <- pnorm(y, mean=100, sd=10)
Gy <- plogis(qlogis(Fy) + log(10))
approx(Gy, y, xout=0.5)$y    # median
# First do this theoretically
# Control arm has Gaussian Y with mean 100, SD 10
# Create experimenetal arm distribution using OR=10
y <- seq(60, 145, length=150)
Fy <- 1 - pnorm(y, mean=100, sd=10)  # P(Y >= y)
Gy <- plogis(qlogis(Fy) + log(10))
approx(Gy, y, xout=0.5)$y    # median
# Theoretical q-q plot: check linearity of inverse normally transformed
# experimental arm distribution
plot(y, qnorm(1 - Gy), type='l')
# First do this theoretically
# Control arm has Gaussian Y with mean 100, SD 10
# Create experimenetal arm distribution using OR=10
y <- seq(60, 145, length=150)
Fy <- 1 - pnorm(y, mean=100, sd=10)  # P(Y >= y)
Gy <- 1 - plogis(qlogis(Fy) + log(10))
approx(Gy, y, xout=0.5)$y    # median
# Theoretical q-q plot: check linearity of inverse normally transformed
# experimental arm distribution
plot(y, qnorm(Gy), type='l')
lines(lsfit(y, qnorm(Gy)))
abline(lsfit(y, qnorm(Gy)))
abline(lsfit(y, qnorm(Gy)), col=gray(.85))
approx(Gy, y, xout=0.5)$y    # median
# Plot new CDF vs. best normal approximation
plot(y, Gy, type='l', ylab=expression(P(Y <= y)))
f <- lsfit(y, qnorm(Gy))
f
coef(f)
lines(y, pnorm(coef(f)[1] + coef(f)[2] * y), col=gray(.85))
# Plot new CDF vs. best normal approximation
plot(y, Gy, type='l', ylab=expression(P(Y <= y)))
f <- lsfit(y, qnorm(Gy))
lines(y, pnorm(coef(f)[1] + coef(f)[2] * y), col=gray(.85))
lines(y, pnorm(qnorm(gy)), col='red')
# Plot new CDF vs. best normal approximation
plot(y, Gy, type='l', ylab=expression(P(Y <= y)))
f <- lsfit(y, qnorm(Gy))
lines(y, pnorm(qnorm(Gy)), col='red')
# First do this theoretically
# Control arm has Gaussian Y with mean 100, SD 10
# Create experimenetal arm distribution using OR=10
y <- seq(60, 145, length=150)
Fy <- 1 - pnorm(y, mean=100, sd=10)  # P(Y >= y)
Gy <- 1 - plogis(qlogis(Fy) + log(10))
approx(Gy, y, xout=0.5)$y    # median
# Plot new CDF vs. best normal approximation
plot(y, Gy, type='l', ylab=expression(P(Y <= y)))
f <- lsfit(y, qnorm(Gy))
coef(f)
f
cor(predict(f, qnorm(Gy)))
# Plot new CDF vs. best normal approximation
plot(y, Gy, type='l', ylab=expression(P(Y <= y)))
f <- lsfit(y, qnorm(Gy))
lines(y, pnorm(qnorm(Gy)), col='red')
lines(y, pnorm(coef(f)[1] + coef(f)[2] * y), col=gray(.85))
plot(Fy, Gy)
approx(Gy, y, xout=0.5)$y    # median
# Theoretical q-q plot: check linearity of inverse normally transformed
# experimental arm distribution
plot(y, qnorm(Gy), type='l')
abline(lsfit(y, qnorm(Gy)), col=gray(.85))
qu <- approx(Gy, y, xout=c(0.25, 0.5, 0.75))$y
qu
s <- (q3 - q1) / (qnorm(0.75) - qnorm(0.25))
qu <- approx(Gy, y, xout=c(0.25, 0.5, 0.75))$y
qu    # Q1, median, Q2
q1 <- qu[1]; q3 <- qu[3]
s <- (q3 - q1) / (qnorm(0.75) - qnorm(0.25))
s
mu <- q1 - s * qnorm(0.25)
mu
# Plot new CDF vs. normal approximation agreeing at quartiles
plot(y, Gy, type='l', ylab=expression(P(Y <= y)))
qu <- approx(Gy, y, xout=c(0.25, 0.5, 0.75))$y
qu    # Q1, median, Q2
q1 <- qu[1]; q3 <- qu[3]
s <- (q3 - q1) / (qnorm(0.75) - qnorm(0.25))
mu <- q1 - s * qnorm(0.25)
lines(y, pnorm(y, mean=mu, sd=s), col=gray(.85))
# Theoretical q-q plot: check linearity of inverse normally transformed
# experimental arm distribution
plot(y, qnorm(Gy), type='l')
abline(lsfit(y, qnorm(Gy)), col=gray(.85))
# First do this theoretically
# Control arm has Gaussian Y with mean 100, SD 10
# Create experimenetal arm distribution using OR=10
y <- seq(60, 145, length=150)
Fy <- 1 - pnorm(y, mean=100, sd=10)  # P(Y >= y)
Gy <- 1 - plogis(qlogis(Fy) + log(10))
# Plot new CDF vs. normal approximation agreeing at quartiles
plot(y, Gy, type='l', ylab=expression(P(Y <= y)))
qu <- approx(Gy, y, xout=c(0.25, 0.5, 0.75))$y
qu    # Q1, median, Q2
q1 <- qu[1]; q3 <- qu[3]
s <- (q3 - q1) / (qnorm(0.75) - qnorm(0.25))
mu <- q1 - s * qnorm(0.25)
lines(y, pnorm(y, mean=mu, sd=s), col=gray(.85))
mu <- q1 - s * qnorm(0.25)
lines(y, pnorm(y, mean=mu, sd=s), col=gray(.85))
# Theoretical q-q plot: check linearity of inverse normally transformed
# experimental arm distribution
plot(y, qnorm(Gy), type='l')
abline(lsfit(y, qnorm(Gy)), col=gray(.85))
# Compute a new discrete distribution if we convert the control
# distribution using proportional odds
# Done by using a discrete distribution with 200,000 points
p <- pomodm(p=rep(1/m, m), odds.ratio=10)
range(p)  # control arm: all 1/200000
max(p)/min(p)
hist(p / min(p))
hist(p / min(p), nclass=200)
range(p)  # control arm: all 1/200000
wtd.mean(y1, p)  # mean shifted by about 12 units
y2 <- rep(y1, each=round(p / min(p)))
?rep
rep(1:3, each=c(2, 4, 6))
rep(1:3, c(2, 4, 6))
y2 <- rep(y1, round(p / min(p)))
length(y2)
mean(y2)
quantile(y2, c(.25, .5, .75))
Ecdf(y2)
lines(y, pnorm(y, mean=mean(y2), sd=sd(y2)), col='blue')
# First do this theoretically
# Control arm has Gaussian Y with mean 100, SD 10
# Create experimenetal arm distribution using OR=10
y <- seq(60, 145, length=150)
Fy <- 1 - pnorm(y, mean=100, sd=10)  # P(Y >= y)
Gy <- 1 - plogis(qlogis(Fy) + log(10))
# Plot new CDF vs. normal approximation agreeing at quartiles
plot(y, Gy, type='l', ylab=expression(P(Y <= y)))
qu <- approx(Gy, y, xout=c(0.25, 0.5, 0.75))$y
qu    # Q1, median, Q2
q1 <- qu[1]; q3 <- qu[3]
s <- (q3 - q1) / (qnorm(0.75) - qnorm(0.25))
mu <- q1 - s * qnorm(0.25)
lines(y, pnorm(y, mean=mu, sd=s), col='blue')
# Theoretical q-q plot: check linearity of inverse normally transformed
# experimental arm distribution
plot(y, qnorm(Gy), type='l')
abline(lsfit(y, qnorm(Gy)), col='blue')
# Compute a new discrete distribution if we convert the control
# distribution using proportional odds
# Done by using a discrete distribution with 200,000 points
p <- pomodm(p=rep(1/m, m), odds.ratio=10)
range(p)  # control arm: all 1/200000
wtd.mean(y1, p)  # mean shifted by about 12 units
y2 <- rep(y1, round(p / min(p)))
mean(y2)
quantile(y2, c(.25, .5, .75))
Ecdf(y2)
lines(y, pnorm(y, mean=mean(y2), sd=sd(y2)), col='blue')
# Form new distribution by repeating each observation a number
# of times equal to the ratio of the new probability to the
# minimum of all new probabilities
y2 <- rep(y1, round(p / min(p)))  # 2M obs instead of 200K
mean(y2)
quantile(y2, c(.25, .5, .75))
# The following plot is virtually the same as the previous one
Ecdf(y2)
lines(y, pnorm(y, mean=mean(y2), sd=sd(y2)), col='blue')
args(Ecdf)
?Ecdf
getHdata(pbc)
xless(describe(pbc))
?spearman2
attach(pbc)
spearman2(bili ~ spiders + hepatom)
spearman2(albumin ~ spiders + hepatom)
spearman2(cholesterol ~ spiders + hepatom)
spearman2(chol ~ spiders + hepatom)
spearman2(protime ~ spiders + hepatom)
spearman2(trig ~ spiders + hepatom)
spearman2(bili ~ spiders + hepatom)
getHdata(pbc)
Ecdf(bili ~ spiders, data=pbc, fun=qlogis)
Ecdf(~ bili, group = spiders, data=pbc, fun=qlogis)
Ecdf(~ bili, group=spiders, data=pbc, fun=qnorm)
# Take logit of ECDF
Ecdf(~ bili, group = spiders, data=pbc, fun=qlogis)
?matrix
ors <- c(1, 1.05, 1.1, 1.2, 1.25, 1.5, 2)
w <- matrix(NA, nrow=length(ors), ncol=3,
ors <- c(1, 1.05, 1.1, 1.2, 1.25, 1.5, 2)
w <- matrix(NA, nrow=length(ors), ncol=3,
dimnames=list(NULL, c('OR', 'mean', 'median')))
ors <- c(1, 1.05, 1.1, 1.2, 1.25, 1.5, 2)
w <- matrix(NA, nrow=length(ors), ncol=3,
dimnames=list(NULL, c('OR', 'mean', 'median')))
w
or=1
z <- pomodm(x, p, odds.ratio=or)
require(Hmisc)
z <- pomodm(x, p, odds.ratio=or)
x <- c(0, 2 ^ (0:6))
z <- pomodm(x, p, odds.ratio=or)
p <- c(.3, rep(.1, 7))
z <- pomodm(x, p, odds.ratio=or)
names(z)
w <- matrix(NA, nrow=length(ors), ncol=2,
dimnames=list(OR=ors), c('OR', 'mean', 'median')))
w <- matrix(NA, nrow=length(ors), ncol=2,
dimnames=list(OR=ors, c('OR', 'mean', 'median')))
w <- matrix(NA, nrow=length(ors), ncol=2,
dimnames=list(OR=ors, c('mean', 'median')))
w
x <- c(0, 2 ^ (0:6))
sum(p * x)  # check mean with OR=1
ors <- c(1, 1.05, 1.1, 1.2, 1.25, 1.5, 2)
w <- matrix(NA, nrow=length(ors), ncol=2,
dimnames=list(OR=ors, c('mean', 'median')))
for(or in ors)
w[or, ] <- pomodm(x, p, odds.ratio=or)
w
i <- 0
for(or in ors) {
i <- i + 1
w[i, ] <- pomodm(x, p, odds.ratio=or)
}
w
.7356 * 3 / pi
set.seed(1)
group <- rep(c('A','B','C','D'), 100)
table(group)
group <- rep(c('A','B','C','D'), 100)
set.seed(1)
group <- rep(c('A','B','C','D'), 100)
y <- rnorm(500, 100, 15) + 10*(group == 'B') + 20*(group=='C') + 30*(group=='D')
tapply(y, group, mean)
set.seed(1)
group <- rep(c('A','B','C','D'), 100)
y <- rnorm(400, 100, 15) + 10*(group == 'B') + 20*(group=='C') + 30*(group=='D')
tapply(y, group, mean)
f <- orm(y ~ group)
require(rms)
f <- orm(y ~ group)
f
# Derive R function to use all intercepts and betas to compute predicted means
M <- Mean(f)
Predict(f, group, fun=M)
dd <- datadist(group, y); options(datadist='dd')
# Derive R function to use all intercepts and betas to compute predicted means
M <- Mean(f)
Predict(f, group, fun=M)
# Compare with sample means
tapply(y, group, mean)
# Compare B and C
contrast(f, list(group='C'), list(group='B'))
?contrast.rms
# Compare B and C
k <- contrast(f, list(group='C'), list(group='B'))
print(k, fun=exp)
k
diff(coef(f))
Ecdf(y, group=group, fun=qnorm)
f <- orm(y ~ group)
warnings(_)
warnings()
f <- orm(y ~ group)
f <- orm(y ~ group)
f <- orm(y ~ group)
f    # use LR chi-square test as replacement for Kruskal-Wallis
Predict(f, group, fun=M)
# Compare B and C
k <- contrast(f, list(group='C'), list(group='B'))
k
# Show odds ratios instead of differences in betas
print(k, fun=exp)
summarize(y, group, smean.cl.normal)
